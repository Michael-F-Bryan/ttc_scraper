{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin\n",
    "import re\n",
    "from pprint import pprint\n",
    "import os\n",
    "import errno\n",
    "from collections import namedtuple\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed, wait\n",
    "\n",
    "import logging\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.DEBUG)\n",
    "\n",
    "logging.getLogger(\"urllib3\").setLevel(logging.WARNING)\n",
    "logging.getLogger(\"requests\").setLevel(logging.WARNING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "_suffixes = ['B', 'KB', 'MB', 'GB', 'TB', 'PB']\n",
    "def humansize(nbytes, decimals=2):\n",
    "    \"\"\"\n",
    "    Convert a number of bytes into it's human readable string using SI \n",
    "    suffixes.\n",
    "\n",
    "    Note\n",
    "    ----\n",
    "    1 KB = 1024 bytes\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    nbytes: int\n",
    "        The total number of bytes\n",
    "    decimals: int\n",
    "        The number of decimal places to round to\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    string\n",
    "        The human readable size.\n",
    "\n",
    "    \"\"\"\n",
    "    if nbytes == 0: return '0 B'\n",
    "    i = 0\n",
    "    while nbytes >= 1024 and i < len(_suffixes)-1:\n",
    "        nbytes /= 1024.\n",
    "        i += 1\n",
    "    f = ('{}'.format(round(nbytes, decimals)))\n",
    "    f = f.rstrip('0').rstrip('.')\n",
    "    return '%s %s' % (f, _suffixes[i])\n",
    "\n",
    "def mkdir(path):\n",
    "    try:\n",
    "        os.makedirs(path)\n",
    "        logger.debug('Made directory: {}'.format(path))\n",
    "    except OSError as e:\n",
    "        if e.errno == errno.EEXIST and os.path.isdir(path):\n",
    "            pass\n",
    "        else:\n",
    "            raise\n",
    "        \n",
    "class BrowserError(RuntimeErroror):\n",
    "    pass\n",
    "\n",
    "class LoginError(BrowserError):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Browser:\n",
    "    base_url = 'http://sae.wsu.edu/ttc/'\n",
    "    \n",
    "    def __init__(self, username, password, save_folder=None, force=False, workers=15, be_courteous=False):\n",
    "        logger.info('Initialising')\n",
    "        \n",
    "        self.username = username\n",
    "        self.password = password\n",
    "        self.save_folder = save_folder or os.path.abspath('./downloads')\n",
    "        self.force = force\n",
    "        self.workers = workers\n",
    "        self.be_courteous = be_courteous\n",
    "        \n",
    "        mkdir(self.save_folder)\n",
    "        \n",
    "        self.session = requests.session()\n",
    "        \n",
    "    def login(self):\n",
    "        \"\"\"Logs into the TTC forum using the provided credentials.\"\"\"\n",
    "        logger.info('Logging in')\n",
    "        \n",
    "        r = self.session.get(self.base_url)\n",
    "        soup = BeautifulSoup(r.text, 'html.parser')\n",
    "            \n",
    "        form = soup.find(class_='quick-login').parent\n",
    "        href = urljoin(self.base_url, form['action'])\n",
    "        \n",
    "        payload = {\n",
    "            'username': self.username,\n",
    "            'password': self.password,\n",
    "            'login': 'Login',\n",
    "        }\n",
    "        # Send the actual login POST request\n",
    "        self.session.post(href, data=payload)\n",
    "        \n",
    "        # Now double check that we logged in successfully\n",
    "        r = self.session.get(urljoin(self.base_url, 'index.php'))\n",
    "        soup = BeautifulSoup(r.text, 'html.parser')\n",
    "        \n",
    "        forum_titles = [x.text for x in soup.find_all(class_='forumtitle')]\n",
    "        if 'All Users Must Register' in forum_titles:\n",
    "            logger.error('Login unsuccessful')\n",
    "            raise LoginError('Login unsuccessful')\n",
    "        else:\n",
    "            logger.info('Login successful')\n",
    "            \n",
    "    def testing_rounds(self):\n",
    "        \"\"\"\n",
    "        Get links to each of the testing rounds' individual forums.\n",
    "        \"\"\"\n",
    "        logger.info('Getting links to each round')\n",
    "        \n",
    "        r = self.session.get(self.base_url + 'index.php')\n",
    "        soup = BeautifulSoup(r.text, 'html.parser')\n",
    "        \n",
    "        forum_titles = soup.find_all(class_='forumtitle')\n",
    "        for title in forum_titles:\n",
    "            if 'Tire Testing' == title.text:\n",
    "                testing_form = urljoin(self.base_url, title['href'])\n",
    "                break\n",
    "        else:\n",
    "            raise RuntimeError('Could not find the \"Tire Testing\" forum. Are you logged in?')\n",
    "            \n",
    "        r = self.session.get(testing_form)\n",
    "        soup = BeautifulSoup(r.text, 'html.parser')\n",
    "        \n",
    "        forum_titles = soup.find_all(class_='forumtitle')\n",
    "        \n",
    "        links = []\n",
    "        for title in forum_titles:\n",
    "            url = urljoin(self.base_url, title['href'])\n",
    "            links.extend(self.get_data_links(url))\n",
    "            \n",
    "        logger.info('{} links found'.format(len(links)))\n",
    "        for link in links:\n",
    "            logger.debug(link)\n",
    "        return links\n",
    "            \n",
    "    def get_data_links(self, forum_url):\n",
    "        \"\"\"\n",
    "        From each testing round's forum, get each of the posts which\n",
    "        contain testing data.\n",
    "        \"\"\"\n",
    "        logger.debug('Getting posts for forum: {}'.format(forum_url))\n",
    "        \n",
    "        pattern = re.compile(r'Round (\\d+) Data$')\n",
    "        r = self.session.get(forum_url)\n",
    "        soup = BeautifulSoup(r.text, 'html.parser')\n",
    "        \n",
    "        topic_titles = soup.find_all(class_='topictitle')\n",
    "        links = []\n",
    "        for title in topic_titles:\n",
    "            res = pattern.search(title.text)\n",
    "            if res is not None:\n",
    "                links.append(title)\n",
    "               \n",
    "        hrefs = [urljoin(self.base_url, x['href']) for x in links]\n",
    "        for href in hrefs:\n",
    "            logger.debug('Post found: {}'.format(href))\n",
    "        return hrefs\n",
    "    \n",
    "    def scrape_round(self, round_url):\n",
    "        \"\"\"\n",
    "        Given a testing round post's url, find all the downloadable files.\n",
    "        \"\"\"\n",
    "        DownloadLink = namedtuple('DownloadLink', ['round', 'filename', 'link'])\n",
    "        \n",
    "        r = self.session.get(round_url)\n",
    "        soup = BeautifulSoup(r.text, 'html.parser')\n",
    "        \n",
    "        round_title = re.search(r'\\d+', soup.find(id='page-body').h2.text).group()\n",
    "        logger.info('Getting download links for round {}'.format(round_title))\n",
    "\n",
    "        download_links = soup.find_all(href=re.compile(r'download'))\n",
    "        links = []\n",
    "        for link in download_links:\n",
    "            temp = DownloadLink(round_title, \n",
    "                                link.text,\n",
    "                                urljoin(self.base_url, link['href']))\n",
    "            links.append(temp)\n",
    "            \n",
    "        for link in links:\n",
    "            logger.debug('Download found: {}'.format(link))\n",
    "        return links\n",
    "    \n",
    "    def download(self, download_link):\n",
    "        \"\"\"\n",
    "        Given a DownloadLink namedtuple, create it's parent \n",
    "        folder and download it.\n",
    "        \n",
    "        Download the entire file and hold it in memory until the\n",
    "        download is complete. This prevents us from having any\n",
    "        incomplete downloads cluttering the filesystem.\n",
    "        \n",
    "        Note\n",
    "        ----\n",
    "        When run in parallel mode with a large number of worker\n",
    "        threads this could potentially use a large amount of RAM.\n",
    "        \"\"\"\n",
    "        logger.info('Downloading {}'.format(download_link))\n",
    "        \n",
    "        download_folder = os.path.join(self.save_folder, \n",
    "                              'Round_{}'.format(download_link.round))\n",
    "        mkdir(download_folder)\n",
    "        filename = os.path.join(download_folder, download_link.filename)\n",
    "        \n",
    "        if os.path.exists(filename) and not self.force:\n",
    "            logger.info('File already exists: {}'.format(filename))\n",
    "            return 0\n",
    "        \n",
    "\n",
    "        r = self.session.get(download_link.link)\n",
    "        \n",
    "        with open(filename, 'wb') as f:\n",
    "            f.write(r.content)\n",
    "        \n",
    "        logger.info('{} downloaded for Round {}, {}'.format(\n",
    "                humansize(len(r.content)),\n",
    "                download_link.round,\n",
    "                download_link.filename))\n",
    "        return len(r.content)\n",
    "    \n",
    "    def _start_sequential(self):\n",
    "        \"\"\"\n",
    "        Do all downloads and page scraping sequentially.\n",
    "        \n",
    "        This is good for rate limiting and when you are either\n",
    "        testing something out or don't want to be mean to the\n",
    "        TTC forum by hogging all its bandwidth.\n",
    "        \"\"\"\n",
    "        logger.inf('Starting in  mode')\n",
    "        \n",
    "        self.login()\n",
    "        round_links = b.testing_rounds()\n",
    "        \n",
    "        files = []\n",
    "        for round_link in round_links:\n",
    "            temp = self.scrape_round(round_link)\n",
    "            files.extend(temp)\n",
    "            \n",
    "        total_downloads = 0\n",
    "        for file in files:\n",
    "            size = self.download(file)\n",
    "            total_downloads += size\n",
    "            \n",
    "        return total_downloads\n",
    "        \n",
    "    def _start_concurrent(self):\n",
    "        \"\"\"\n",
    "        Do all page scraping and downloads in parallel.\n",
    "        \n",
    "        This is the default download method, it's good if\n",
    "        you have fast download speed and if you want to\n",
    "        download everything as quickly as possible.\n",
    "        \"\"\"\n",
    "        logger.info('Starting in concurrent mode')\n",
    "        \n",
    "        pool = ThreadPoolExecutor(max_workers=self.workers)\n",
    "        self.login()\n",
    "        round_links = b.testing_rounds()\n",
    "        \n",
    "        futures = []\n",
    "        for link in round_links:\n",
    "            fut = pool.submit(self.scrape_round, link)\n",
    "            futures.append(fut)\n",
    "            \n",
    "        download_links = []\n",
    "        for future in as_completed(futures):\n",
    "            download_links.extend(future.result())\n",
    "            \n",
    "        futures = []\n",
    "        for download_link in download_links:\n",
    "            fut = pool.submit(self.download, download_link)\n",
    "            futures.append(fut)\n",
    "            \n",
    "        total_bytes = 0\n",
    "        for future in as_completed(futures):\n",
    "            download_size = future.result()\n",
    "            total_bytes += download_size\n",
    "            print(download_size)\n",
    "            \n",
    "        logger.info('Total bytes downloaded: {}'.format(humansize(total_bytes)))\n",
    "        return total_bytes\n",
    "\n",
    "    def start(self):\n",
    "        \"\"\"\n",
    "        A proxy for Browser._start_sequential or Browser._start_concurrent\n",
    "        depending on the value of `be_courteous`.\n",
    "        \"\"\"\n",
    "        if be_courteous:\n",
    "            self.start = self._start_sequential\n",
    "        else:\n",
    "            self.start = self._start_concurrent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n",
      "0\n",
      "7817\n",
      "11393\n",
      "34756\n",
      "400787\n",
      "153501\n",
      "748907\n",
      "11649024\n",
      "21370699\n",
      "27920806\n",
      "29383870\n",
      "30767025\n",
      "31224665\n",
      "35509583\n",
      "16299\n",
      "7902\n",
      "37404736\n",
      "39873260\n",
      "305277\n",
      "649810\n",
      "42974272\n",
      "41554656\n",
      "44713965\n",
      "213504\n",
      "8004733\n",
      "37174784\n",
      "51158253\n",
      "11767217\n",
      "453109\n",
      "11526174\n",
      "10255612\n",
      "7902\n",
      "16522\n",
      "305898\n",
      "34027\n",
      "0\n",
      "168960\n",
      "11393\n",
      "8215\n",
      "53558644\n",
      "432379\n",
      "19172393\n",
      "8844707\n",
      "7500903\n",
      "11072986\n",
      "12608815\n",
      "33025192\n",
      "34729941\n",
      "45467852\n",
      "35519092\n",
      "19158582\n",
      "42781571\n",
      "23160782\n",
      "25770442\n",
      "24437297\n",
      "39068257\n",
      "40787873\n",
      "47548680\n",
      "49649535\n",
      "50357499\n",
      "47930064\n",
      "48658502\n",
      "57836630\n",
      "111849609\n",
      "47791670\n",
      "60281674\n",
      "46678090\n",
      "45630221\n",
      "21897154\n",
      "20542623\n",
      "20665736\n",
      "24088011\n",
      "19710040\n",
      "24951487\n",
      "30491812\n",
      "32397577\n",
      "36180452\n",
      "38032535\n",
      "33177358\n",
      "34718364\n",
      "40334832\n",
      "41824096\n",
      "44793880\n",
      "47927439\n",
      "57951712\n",
      "50783735\n",
      "56608521\n",
      "53585426\n",
      "60342872\n",
      "43818214\n",
      "44898505\n",
      "48828829\n",
      "18140936\n",
      "21231\n",
      "5052299\n",
      "1202334\n",
      "70123068\n",
      "74922587\n",
      "83272874\n",
      "87862048\n",
      "86344729\n",
      "71707228\n",
      "75379489\n",
      "83143622\n",
      "80882408\n",
      "84527531\n",
      "92561312\n",
      "251744950\n",
      "449839966\n"
     ]
    }
   ],
   "source": [
    "username = 'kyleaurisch'\n",
    "password = 'lancer12'\n",
    "\n",
    "b = Browser(username, password)\n",
    "b.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
