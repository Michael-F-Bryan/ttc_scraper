{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Browser:\n",
    "    base_url = 'http://sae.wsu.edu/ttc/'\n",
    "    \n",
    "    def __init__(self, username, password, save_folder=None, force=False, workers=15, be_courteous=False):\n",
    "        logger.info('Initialising')\n",
    "        \n",
    "        self.username = username\n",
    "        self.password = password\n",
    "        self.save_folder = save_folder or os.path.abspath('./downloads')\n",
    "        self.force = force\n",
    "        self.workers = workers\n",
    "        self.be_courteous = be_courteous\n",
    "        \n",
    "        mkdir(self.save_folder)\n",
    "        \n",
    "        self.session = requests.session()\n",
    "        \n",
    "    def login(self):\n",
    "        \"\"\"Logs into the TTC forum using the provided credentials.\"\"\"\n",
    "        logger.info('Logging in')\n",
    "        \n",
    "        r = self.session.get(self.base_url)\n",
    "        soup = BeautifulSoup(r.text, 'html.parser')\n",
    "            \n",
    "        form = soup.find(class_='quick-login').parent\n",
    "        href = urljoin(self.base_url, form['action'])\n",
    "        \n",
    "        payload = {\n",
    "            'username': self.username,\n",
    "            'password': self.password,\n",
    "            'login': 'Login',\n",
    "        }\n",
    "        # Send the actual login POST request\n",
    "        self.session.post(href, data=payload)\n",
    "        \n",
    "        # Now double check that we logged in successfully\n",
    "        r = self.session.get(urljoin(self.base_url, 'index.php'))\n",
    "        soup = BeautifulSoup(r.text, 'html.parser')\n",
    "        \n",
    "        forum_titles = [x.text for x in soup.find_all(class_='forumtitle')]\n",
    "        if 'All Users Must Register' in forum_titles:\n",
    "            logger.error('Login unsuccessful')\n",
    "            raise LoginError('Login unsuccessful')\n",
    "        else:\n",
    "            logger.info('Login successful')\n",
    "            \n",
    "    def testing_rounds(self):\n",
    "        \"\"\"\n",
    "        Get links to each of the testing rounds' individual forums.\n",
    "        \"\"\"\n",
    "        logger.info('Getting links to each round')\n",
    "        \n",
    "        r = self.session.get(self.base_url + 'index.php')\n",
    "        soup = BeautifulSoup(r.text, 'html.parser')\n",
    "        \n",
    "        forum_titles = soup.find_all(class_='forumtitle')\n",
    "        for title in forum_titles:\n",
    "            if 'Tire Testing' == title.text:\n",
    "                testing_form = urljoin(self.base_url, title['href'])\n",
    "                break\n",
    "        else:\n",
    "            raise RuntimeError('Could not find the \"Tire Testing\" forum. Are you logged in?')\n",
    "            \n",
    "        r = self.session.get(testing_form)\n",
    "        soup = BeautifulSoup(r.text, 'html.parser')\n",
    "        \n",
    "        forum_titles = soup.find_all(class_='forumtitle')\n",
    "        \n",
    "        links = []\n",
    "        for title in forum_titles:\n",
    "            url = urljoin(self.base_url, title['href'])\n",
    "            links.extend(self.get_data_links(url))\n",
    "            \n",
    "        logger.info('{} links found'.format(len(links)))\n",
    "        for link in links:\n",
    "            logger.debug(link)\n",
    "        return links\n",
    "            \n",
    "    def get_data_links(self, forum_url):\n",
    "        \"\"\"\n",
    "        From each testing round's forum, get each of the posts which\n",
    "        contain testing data.\n",
    "        \"\"\"\n",
    "        logger.debug('Getting posts for forum: {}'.format(forum_url))\n",
    "        \n",
    "        pattern = re.compile(r'Round (\\d+) Data$')\n",
    "        r = self.session.get(forum_url)\n",
    "        soup = BeautifulSoup(r.text, 'html.parser')\n",
    "        \n",
    "        topic_titles = soup.find_all(class_='topictitle')\n",
    "        links = []\n",
    "        for title in topic_titles:\n",
    "            res = pattern.search(title.text)\n",
    "            if res is not None:\n",
    "                links.append(title)\n",
    "               \n",
    "        hrefs = [urljoin(self.base_url, x['href']) for x in links]\n",
    "        for href in hrefs:\n",
    "            logger.debug('Post found: {}'.format(href))\n",
    "        return hrefs\n",
    "    \n",
    "    def scrape_round(self, round_url):\n",
    "        \"\"\"\n",
    "        Given a testing round post's url, find all the downloadable files.\n",
    "        \"\"\"\n",
    "        DownloadLink = namedtuple('DownloadLink', ['round', 'filename', 'link'])\n",
    "        \n",
    "        r = self.session.get(round_url)\n",
    "        soup = BeautifulSoup(r.text, 'html.parser')\n",
    "        \n",
    "        round_title = re.search(r'\\d+', soup.find(id='page-body').h2.text).group()\n",
    "        logger.info('Getting download links for round {}'.format(round_title))\n",
    "\n",
    "        download_links = soup.find_all(href=re.compile(r'download'))\n",
    "        links = []\n",
    "        for link in download_links:\n",
    "            temp = DownloadLink(round_title, \n",
    "                                link.text,\n",
    "                                urljoin(self.base_url, link['href']))\n",
    "            links.append(temp)\n",
    "            \n",
    "        for link in links:\n",
    "            logger.debug('Download found: {}'.format(link))\n",
    "        return links\n",
    "    \n",
    "    def download(self, download_link):\n",
    "        \"\"\"\n",
    "        Given a DownloadLink namedtuple, create it's parent \n",
    "        folder and download it.\n",
    "        \n",
    "        Download the entire file and hold it in memory until the\n",
    "        download is complete. This prevents us from having any\n",
    "        incomplete downloads cluttering the filesystem.\n",
    "        \n",
    "        Note\n",
    "        ----\n",
    "        When run in parallel mode with a large number of worker\n",
    "        threads this could potentially use a large amount of RAM.\n",
    "        \"\"\"\n",
    "        logger.info('Downloading {}'.format(download_link))\n",
    "        \n",
    "        download_folder = os.path.join(self.save_folder, \n",
    "                              'Round_{}'.format(download_link.round))\n",
    "        mkdir(download_folder)\n",
    "        filename = os.path.join(download_folder, download_link.filename)\n",
    "        \n",
    "        if os.path.exists(filename) and not self.force:\n",
    "            logger.info('File already exists: {}'.format(filename))\n",
    "            return 0\n",
    "        \n",
    "\n",
    "        r = self.session.get(download_link.link)\n",
    "        \n",
    "        with open(filename, 'wb') as f:\n",
    "            f.write(r.content)\n",
    "        \n",
    "        logger.info('{} downloaded for Round {}, {}'.format(\n",
    "                humansize(len(r.content)),\n",
    "                download_link.round,\n",
    "                download_link.filename))\n",
    "        return len(r.content)\n",
    "    \n",
    "    def _start_sequential(self):\n",
    "        \"\"\"\n",
    "        Do all downloads and page scraping sequentially.\n",
    "        \n",
    "        This is good for rate limiting and when you are either\n",
    "        testing something out or don't want to be mean to the\n",
    "        TTC forum by hogging all its bandwidth.\n",
    "        \"\"\"\n",
    "        logger.inf('Starting in  mode')\n",
    "        \n",
    "        self.login()\n",
    "        round_links = b.testing_rounds()\n",
    "        \n",
    "        files = []\n",
    "        for round_link in round_links:\n",
    "            temp = self.scrape_round(round_link)\n",
    "            files.extend(temp)\n",
    "            \n",
    "        total_downloads = 0\n",
    "        for file in files:\n",
    "            size = self.download(file)\n",
    "            total_downloads += size\n",
    "            \n",
    "        return total_downloads\n",
    "        \n",
    "    def _start_concurrent(self):\n",
    "        \"\"\"\n",
    "        Do all page scraping and downloads in parallel.\n",
    "        \n",
    "        This is the default download method, it's good if\n",
    "        you have fast download speed and if you want to\n",
    "        download everything as quickly as possible.\n",
    "        \"\"\"\n",
    "        logger.info('Starting in concurrent mode')\n",
    "        \n",
    "        pool = ThreadPoolExecutor(max_workers=self.workers)\n",
    "        self.login()\n",
    "        round_links = b.testing_rounds()\n",
    "        \n",
    "        futures = []\n",
    "        for link in round_links:\n",
    "            fut = pool.submit(self.scrape_round, link)\n",
    "            futures.append(fut)\n",
    "            \n",
    "        download_links = []\n",
    "        for future in as_completed(futures):\n",
    "            download_links.extend(future.result())\n",
    "            \n",
    "        futures = []\n",
    "        for download_link in download_links:\n",
    "            fut = pool.submit(self.download, download_link)\n",
    "            futures.append(fut)\n",
    "            \n",
    "        total_bytes = 0\n",
    "        for future in as_completed(futures):\n",
    "            download_size = future.result()\n",
    "            total_bytes += download_size\n",
    "            print(download_size)\n",
    "            \n",
    "        logger.info('Total bytes downloaded: {}'.format(humansize(total_bytes)))\n",
    "        return total_bytes\n",
    "\n",
    "    def start(self):\n",
    "        \"\"\"\n",
    "        A proxy for Browser._start_sequential or Browser._start_concurrent\n",
    "        depending on the value of `be_courteous`.\n",
    "        \"\"\"\n",
    "        if be_courteous:\n",
    "            self.start = self._start_sequential\n",
    "        else:\n",
    "            self.start = self._start_concurrent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from ttc_scraper import Browser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "username = 'kyleaurisch'\n",
    "password = 'lancer12'\n",
    "\n",
    "b = Browser(username, password)\n",
    "b.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
